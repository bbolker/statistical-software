---
title: 'rOpenSci Statistical Software Review: Scoping Document'
author: 'Noam Ross and Mark Padgham'
date: '`r Sys.Date()`'
output:
  html_document:
    toc: yes
    toc_float: yes
    number_sections: no
    theme: flatly
bibliography: statsoft.bib
---

```{r getbib, echo = FALSE}
if (!file.exists ("statsoft.bib")) {
    refs <- RefManageR::ReadZotero(group = "2416765",
                                  .params = list (limit = 100))
    RefManageR::WriteBib(refs, "statsoft.bib")
}
```

Introduction
============

Background
----------

This document serves as an organizing framework for rOpenSci's project for
peer-review of statistical software. It lays out key considerations, outstanding
questions, and tasks for our first year of work, for purposes of generating
community feedback.

Project Aims
------------

-   To foster a community of practice in which users and developers of
    statistical software mutually improve quality, reproducibility, and
    reliability of research.

-   To provide software creators with a set of tools to assess the quality of
    their work, and a process by which to improve it.

-   To provide statistical software developers, users, and consumers of results
    a discoverable "badge" that transparently conveys a level of assurance of
    software quality and may be usable as professional credit.

-   To create a set of standards that may be adopted and adapted by open source
    and private groups, academic journals, or other statistical software
    evaluation projects.

-   To focus on R as primary language, but separate language-specific from
    language- agnostic components so as to maximize adaptability to other
    contexts.

-   To focus on problems and solutions specific to statistical software.

Related projects and initiatives
--------------------------------

The following internal and external projects related projects have bearing on
our work:

rOpenSci is simultaneously working on improving the automation and documentation
of infrastructure to support software peer reviews. This will support many of
the processes described herein, initially those in metrics and diagnostic
reports (below), as well as in managing the review process.

Secondly, [under a separate project funded by the Moore
Foundation](https://ropensci.org/blog/2019/11/06/scientific-package-ecosystem/),
rOpenSci is building a system to automate the retrieval of information related
to use of software in published literature and automate reporting of software
impact as part of metadata in software repositories. This builds on
[Depsy](http://depsy.org/) and [CiteAs](http://citeas.org/about) projects and
may be leveraged for our work on metrics (below).

Third, an initiative organized under the R Consortium, the [R Validation
Hub](https://www.pharmar.org/), seeks to provide guidance primarily to R users
in the pharmaceutical industry on validating R packages in regulated contexts.
We are working closely with that group to share products and avoid duplicated
efforts.

Scope of Statistical Software Review
====================================

A core task is to define the set of software that will be covered by our review
process and standards.

-   What categories of statistical software might be considered *in scope*?

-   What categories of statistical software might be considered *out of scope*?

-   How would these vary between a useful definition and typology for general
    use, and the specific case of our R-focused peer review system?

A key consideration in scope is identifying categories of software that (a) will
benefit from our peer review process, and (b) the review process will be
equipped to handle. That is, can standards and procedures be defined that are
applicable, and will reviewers be able to apply them?

In considering these issues of definition, it will be important to consider
whether it may be advantageous or necessary to develop different procedures for
different sub-categories, whether in terms of categories of external form or
categories of statistical software. It may suffice, at least initially, to
simply provide broad definitions of what might lie *beyond* the scope of the
present project.

Defining "Statistical Software"
-------------------------------

There is no ready definition for "statistical software", but nor may such a
definition be essential to the success of the present project. Some but not all
categories that may be included are

-   Supervised / regression models and algorithms

-   Unsupervised models and algorithms

-   Predictive / black box approaches

-   Parametric and analytically tractable approaches

-   Implementation of new methods

-   Re-implementation of methods

-   Workflow support for multiple methods or specific contexts

-   Summary statistic calculation

-   Statistical visualization and diagnostics

-   Power analysis and study design

As a point of comparison, the Journal of Statistical Software journal defines
[its own scope](https://www.jstatsoft.org/pages/view/mission) as "statistical
computing in all areas of empirical research," with articles describing
"comprehensive open-source implementations of broad classes of statistical
models and procedures or computational infrastructure upon which such
implementations can be built."[^1]

Computer Languages and Package Structures
-----------------------------------------

Our scope of work for this project is focused on developing peer-review for
statistical software in the R language. This most likely will refer to R
*packages*. However, it may also consider review of other forms for bundling R
software, or software primarily written in other languages. Moreover, R packages
often contain code from a variety of other languages, traditionally Fortran and
C, now very commonly also C++, as well as other non-compiled languages such as
JavaScript, and compiled languages such as Rust. It is accordingly expected that
software review can potentially encompass code in several languages. We will
need to determine the extent to which each of these categories may be in-scope
and how review may vary between them:

-   R packages containing compiled code in traditionally used compiled languages
    (Fortran C, C++)

-   R packages containing code in other languages (Rust, Python, JavaScript)

-   Packaging appropriate for other languages (such as Python) yet with some
    interface with the R language

-   R interfaces to algorithms or software developed independently in different
    languages ("wrapper" packages)

-   Other forms of bundling or presenting R code other than standard packages
    (scripts, modules, graphical / web user interfaces)

Moreover, the project aims to develop a set of language-independent standards
able to be transferred to other languages and systems for peer review. In
scoping and standards development, we will separate language-agnostic concepts
from language-specific implantation.

Statistical Software Peer Review Process
========================================

Our point of departure for our process is the rOpenSci software peer review
process, which has operated for five years, reviewing \>200 packages primarily
in the area of data lifecycle management. However, we aim to reassess this
process in light of other models and needs specific to statistical software.
Some core questions we seek to resolve are:

-   Are we reviewing full packages or only limited pieces of packages?

-   What is the outcome of review? Binary, rating, checklist,
    acceptance/rejection?

-   To what extent should the review process be automated or self-certified?

-   Which parts of the process should be open and which closed?

-   Should review be a one-off phenomenon, or should there be multiple review
    phases throughout the software lifecycle?

-   Should we maintain the separation or independence of reviewers from code
    development, or might it be better to encourage direct engagement of
    reviewers with ongoing code development?

-   Who should be in the pool of software reviewers and editors?

Current Models
--------------

rOpenSci's current software peer-review process, detailed in our [developer
guide](https://devguide.ropensci.org/softwarereviewintro.html), is based on a
blend of practices from peer review of academic practices and code review in
open-source projects. Review takes place via an issue thread in our
["software-review" repository on
GitHub](https://github.com/ropensci/software-review). The review process is
entirely open, with each issue thread used to manage the entire process,
coordinated by rOpenSci's editors. After initial screening for scope and minimal
qualification by editors two reviewers provide comments and feedback on software
packages. After one or more rounds of revisions, packages reach a point of
approval, at which point they are "accepted " by rOpenSci, symbolized both
through a badge system, and (generally) through transferring the software from
an author's private domain to the [github.com/ropensci
domain](https://github.com/ropensci).

The [Journal of Open Source Software](https://joss.theoj.org/) follow a similar
approach as it was based on rOpenSci, with greater automation and broader scope.
The Journal of Statistical Software conducts a closed review of both manuscript
and software, with fewer prescriptive standards. BioConductor, in reviewing
packages for acceptance into its repository conducts an [open
review](https://www.bioconductor.org/developers/package-submission/) primarily
aimed at maintaining minimum standards and intercompatibilty.

Other initiatives further afield from academic peer review may offer useful
models. For instance, the Linux [Core Infrastructure
Initiative](https://www.coreinfrastructure.org/) provides badges to project
meeting [development best
practices](https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/criteria.md).
Badges are graded (passing/silver/gold), and awarded by package authors
self-certifying that they have implemented items on a checklist.

Software Life Cycle Considerations
----------------------------------

A long history and tradition in both practice and published literature on
software review [for example, @mili_software_2015;@ammann_introduction_2017] generally concludes that software review
is most effective when it is an ongoing process that occurs as frequently as
possible---a practice which contrasts strongly with the singular nature of
review as currently implemented by rOpenSci. Other systems, such as pull-request
reviews or inline systems (e.g.,
[watson-ruby](https://github.com/nhmood/watson-ruby)), focus review on much more
granular scale both in terms of the scale of code reviewed and time frame.

An effective system for peer review of statistical software is thus may lie
somewhere between the "one-off" practices above, and frequent, ongoing review
typical of software development in active teams. An [analysis of the effects of
rOpenSci's review process on a few metrics of software development
activity](https://github.com/mpadge/statistical-software/tree/master/ros-review-effects)
revealed that software development tends to stagnate following review. This may
be interpreted to reflect software having reached a sufficiently stable state.
However, we note that metrics of community engagement with software are
generally positively related to the metrics of development activity considered
there. Slowing of software development following review may also result in
decreases in community engagement.

In addition, ongoing "review" may be explicit in considering the role of user
feedback, for instance, in defining and updating the scope of statistical
routines (see "statistical standards" below).

Standards for Statistical Software
==================================

An important output of the present project is anticipated to be two set of
"standards", one pertaining to processes for peer review of software, and one
pertaining to the software itself. The former of these is only indirectly
addressed within this document, through the preceding sections, with concrete
standards expected to emerge from latter phases of the project. The latter
aspect of software standards is the subject of the present section.

Rather than aiming for a fixed set of immutable standards, we acknowledge that
standards of the kind anticipated here will likely be better conceived of to
reflect ongoing processes of development. As such, of equal importance to
developing a set of standards *per se* will be developing an understanding of
the kinds of *processes* which may have the most defining effect on resultant
standards at any point in time. An equally important consideration is the
distinction between pro- and retro-spective standards, where *retrospective
standards* can be applied to any extant software in order to assess the extent
to whether it meets those standards, while *prospective standards* represent a
set of requirements which software is expected to fulfil. The extent to which
standards developed for this project are retrospective versus prospective is
likely to change throughout the project's lifespan and beyond.

Standards are likely to emerge from a variety of concrete metrics of the types
considered in the following section ("*Specific Aspects of Software*"), and are
not considered in any technical detail in the present section. Important
*general* questions regarding standards include the following:

-   What kind of standards might apply to software in general?

-   How might such standards differ between different languages?

-   What kind of standards might specifically apply to statistical software?
    (See the following sub-section.)

-   In what ways might any of the above differ with regard to retro- versus
    pro-spective standards?

-   To what extent should we aim for "verification" or "validation" of software?

General and Specific Standards
------------------------------

Many of the metrics described in the following section can be "automatically"
quantified, and thus procedures for assessing the extent to which software meets
a given sets of standards are readily automated. Doing so nevertheless entails
some degree of risk which it is important both to apperceive, and to consider
within the present general context of standards. Such risk is particularly
apparent with regard to the relationship between software categories and
standards---although these kinds of considerations of risk are in no way
restricted to this domain only.

The applicability of any concrete set of standards is likely to differ between
different categories. In terms of concrete metrics, for example, metrics of
numerical accuracy will differ between categories primarily describing
analytical algorithms and those describing less tractable routines which produce
less directly reproducible results. Or consider metrics derived from tests,
which must be interpreted in *qualitatively* different ways for packages
entirely dependent on their own internal code versus packages largely dependent
on the results of calls to external data providers (along with additional
differences between, for example, locally-installed "external" providers versus
online sources of external data).

Different metrics, and thus by extension very likely different standards, must
thus be considered to be of differential applicability for different categories
of software, and thus the interplay between the scope of statistical software
considered above and throughout this project, and the standards emerging from
the project, will be of critical importance throughout the project. More
concretely, attempts to devise a concrete categorization of statistical software
will inevitably frustrate attempts to understand more *general* processes
leading to the establishment of standards, and therefore negatively impact on
the project's desired goal stated at the outset of creating and providing a
truly *general* and *transferrable* set of standards. Conversely, attempts to
counter this effect by striving for more generally applicable standards will
likely weaken abilities to assess particular categories of statistical software.
Such considerations lead to the following kinds of questions which will likely
have to be addressed:

-   To what extent ought we aim for general standards at the expense of specific
    ability to assess particular categories of statistical software?

-   To what extent ought we strive for automation of software assessment, given
    the inherent risk of overseeing qualitative differences between different
    categories?

-   How much effort should be expended both developing a categorization of
    statistical software, and understanding the potential effects of such a
    categorization?

### Statistical Standards

-   Numerical issues

-   Method validity (i.e., is the method itself valid, independent of
    implementation? Has to do with, perhaps, whether there's literature
    supporting the method.)

-   Scope of applicability of the software / method

-   Is a submission intended to support future or subsequent publications?

Specific Aspects of Software
============================

To additionally inform the general considerations described in the preceding
sections, this section presents a general (yet non-exhaustive) overview of the
kinds of metrics which may be usefully considered in assessing statistical
software, and which may be usefully employed in deriving a set of standards for
such software. This section is primarily intended to address the two directly
related questions of the extent to which the kinds of metrics described might be
useful to both software developers, and to reviewers.

### Software Interface

There are likely aspects of overall software "design" that might be considered,
reviewed, encouraged, or expected. rOpenSci's guide on [package development,
maintenance, and peer review](https://devguide.ropensci.org/) provides arguably
one of the most prominent guides on the design of R packages, primarily with its
first chapter. One of the few other notable examples of guides to design
principles of R packages is the [tidyverse design
guide](https://principles.tidyverse.org/). The following useful list of design
principles is provided by Mili (2015):

-   Functional Attributes

    -   Correctness

    -   Robustness

-   Useability Attributes

    -   Ease of Use

    -   Ease of Learning

    -   Customizability

    -   Calibrability

    -   Interoperability

-   Structural Attributes

    -   Design Integrity

    -   Modularity

    -   Testability

    -   Adaptability

5.1a Key Considerations

-   Might there be an advantage to explicitly considering some of these aspects
    of general software design?

<!-- -->

-   If so, which?

-   At what point in the general lifecycle of software or review might such
    design aspects best be considered or integrated?

### Documentation

In contrast to scientific, software in general, statistical software might be
argued to be associated with some specific kinds of input and output data. This
observation alone suggests the likely importance of some kind of metadata
associated with statistical software which documents the nature of (expected,
permitted, or accepted) input and output data (**???**).

-   Standard software documentation metrics:

    -   Numbers of lines per function

    -   proportion of documentation to code lines

    -   presence of examples

    -   coverage of examples

    -   vignettes

### Testing

Vogel (2011) states: Software that

depends on testing alone for a defect-free

$$\text{state}$$

is depending on perfection in testing

There are no unambiguous categories of tests, but the structure of R packages
which contain both external (exported) and internal (non-exported) functions
provides for a convenient distinction between "unit tests" as tests of
*internal* functions, and *functional* or *integration tests* as tests of
*exported* functions. Almost all testing as currently implemented in R is
"concrete testing" (Mili 2015), and little consideration has been given in R to
"stochastic" or "property-based" testing, in which expectation values of inputs
and outputs are tested, rather than concrete instantiations of such. Other
languages have developed grammars for stochastic or property-based testing,
notably through the [hypothesis package for
python](https://github.com/HypothesisWorks/hypothesis). These grammars enable
specification of test assumptions as well as expected test outputs. Assumptions
in `hypothesis` are declared through simple
`[@``given]{``.redoc #redoc-citation-2}` statements that might, for example,
quantify an assumed probability distribution for input data, while outputs are
specified through equivalent `[@expect]{.redoc #redoc-citation-3}` statements
that might, for example, specify expected *distributional properties* of an
output rather than just concrete values.

5.3a Key considerations

-   To what extend should testing focus on *functional* rather than *unit*
    testing?

<!-- -->

-   What test reporter should be used? The `testthat` package and similar, or
    might it be worth considering new test reporting systems?

-   Is it sufficient to consider test execution as an integral part of
    `R CMD check` only? Or might there by a case for developing alternative test
    execution environments and approaches? For instance, should there be an
    alternate workflow for long-running tests, tests requiring large data, or
    tests intended to be executed for purposes other than (e.g., scope
    delineation).

-   Is it worthwhile concretely defining one or more goals of testing? (Such as
    error detection, error frequencies, error tolerance, accuracy.)

-   What are the test data? And how easy is it to input alternative data to
    tests?

-   Is there scope for "stochastic" or "property-based" testing?

Community
=========

A core goal of the project is the building and maintenance of a community of
practice that will facilitate dissemination adoption, and improvement of
standards and peer review.

-   What outreach should we conduct to maximize diversity and inclusion in the
    process?

-   How should this process involve other relevant communities in fields
    including software development, statistics, applied statistics (in various
    subfields)

-   What fora should we manage for developers, users, reviewers and editors to
    communicate? To what extent should we reuse existing fora from rOpenSci or
    other organizations?

Metrics
=======

-   Code structure

    -   Cyclomatic complexity

    -   Codebase size

    -   Function size / number

    -   Numbers of external calls within functions

    -   Exported / non exported functions

    -   Code consistency

-   Documentation metrics detailed above

-   Meta struture

    -   Dependencies

    -   Reverse dependencies

    -   License

-   Meta metrics

    -   Version control?

    -   Availability of website

    -   Availability of source code (beyond CRAN or similar)

    -   Community:

        -   Software downloads and usage statistics

        -   Numbers of active contributors

        -   Numbers or rates of issues reported

    -   Maintenance:

        -   Rate/Numbers of releases

        -   Rate of response to reported issues

        -   Last commit

        -   Commit rate

    -   stars

    -   forks

-   Extent of testing

    -   Code coverage

    -   Examples

    -   Range of inputs tested

-   Dynamic metrics derived from function call networks

Automation and Tooling
======================

The ultimate question which we would like to answer throughout such
considerations concerns the kinds of tools this project might best focus on
developing, whether those tools be:

-   Qualitative descriptions and understanding of effects of the kinds of
    compromises described above

    -   Tools useful in assessing or formalizing categories of software

-   Quantitative tools to *retrospectively* assess such aspects as:

    -   Software "quality" (see the subsequent section)

    -   Community engagement

    -   Effectiveness (or other metrics) of review

Other aspects pertinent to the general process of peer reviewing open source
statistical software

-   Quantitative tools that can be *prospectively* used to

    -   Improve or assure software quality

    -   Document aspects of software quality

    -   Aid modularity or transferability either of software, or of the tools
        themselves

-   Formalize structural aspects of software such as tests (for example, through
    implementing new frameworks or grammars)

-   Extensions of packages such as **lintr**, **covr**, **goodpractice**

-   Comparisons of package metrics to distributions for other packages

-   Diagnostic and report aggregation, design, automatic creation

There are many stages associated with software development and peer-review that
may effectively be automated, and aspects of such automation is expected to be
integrated within the present project. An important question throughout the
remainder of this document is accordingly the extent to which automation may
enhance either the development or implementation of the various stages
considered below. A good example for the effectiveness of automation in the
kinds of peer review processes envisioned to emerge from this process is
provided by submissions to the [Journal of Open Source
Software](https://joss.theoj.org/), which features [open
reviews](https://github.com/openjournals/joss-reviews/issues), many aspects of
which are automated by a custom-developed bot called
["whedon"](https://github.com/whedon).

Reviewer pool and qualifications:

-   What is the extent and type of effort expected of reviewers?

-   To what extent might searches for suitable reviewers be automated?

-   What sort of metrics might be useful in such searches?

Annotated Bibliography
======================

Lots to put here, but some in addition to stuff in current document:

-   <https://www.alexpghayes.com/blog/type-stable-estimation/>,
    <https://www.alexpghayes.com/blog/testing-statistical-software/>

-   <https://tidymodels.github.io/model-implementation-principles/>

-   <https://github.com/pharmaR/white_paper>

-   <https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/criteria.md>

Ammann, Paul, and Jeff Offutt. 2017. *Introduction to Software Testing*.
Cambridge University Press.

Mili, Ali. 2015. *Software Testing: Concepts and Operations*.

Vogel, David A. 2011. *Medical Device Software Verification, Validation and
Compliance*. Boston: Artech House. <http://site.ebrary.com/id/10436227>.

Ammann, Paul, and Jeff Offutt. 2017. *Introduction to Software Testing*.
Cambridge University Press.

Mili, Ali. 2015. *Software Testing: Concepts and Operations*.

[^1]: We explored whether we could usefully define topics of interest in a
    [preliminary text
    analysis](https://github.com/mpadge/statistical-software/tree/master/jss) of
    all historical submissions to JSS nit found no notable phrases or topics
    which might be useful to define or categorize statistical software, other
    than topics pertaining to particular areas of application.
