---
title: "rOpenSci Statistical Software Review: Scoping Document"
author: "Noam Ross and Mark Padgham"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: true
        number_sections: false
        theme: flatly
bibliography: statsoft.bib
---

<!-- uncomment to render title in github markdown
# rOpenSci Statistical Software Review: Scoping Document
-->

```{r getbib, echo = FALSE}
if (!file.exists ("statsoft.bib")) {
    refs <- RefManageR::ReadZotero(group = "2416765",
                                  .params = list (limit = 100))
    RefManageR::WriteBib(refs, "statsoft.bib")
}
```

# 1. Introduction

This document primarily serves to pose a number of questions regarding
rOpenSci's incipient project for peer-review of statistical software. The
organization itself is, and is likely to largely remain, primarily focussed on
the R language for statistical computing. While the project under consideration
here will likely evolve towards a system for peer review of *R packages*, it
may also consider review of other forms for bundling R software, or other
languages. Moreover, R packages often contain code from a variety of other
languages, traditionally Fortran and C, now very commonly also C++, as well as
other non-compiled languages such as javascript, and compiled languages such as
Rust. It is accordingly expected that software review will encompass several
languages, whether or not bundled in the form of R packages. Moreover, the
project aims to develop a set of language-independent standards able to be
transferred to other languages and systems for peer review.

Peer-review plays a key role in ensuring the sustainability of scientific
software [@downs_community_2015]. rOpenSci has a well-developed system for peer
review of R packages, primarily through their ["software-review" repository on
github](https://github.com/ropensci/software-review), to which packages may be
submitted by opening an issue on that github repository. The review process is
entirely open, with each issue used to manage the entire process, coordinated
by rOpenSci's own editorial committee. Two features of this peer review system
in its current state are important to note here:

1. Statistical software is explicitly *excluded* from current scope; and
2. Tasks associated with peer review are not automated, and require manual
   control and determination of each step.

There are many stages associated with software development and peer-review that
may effectively be automated, and aspects of such automation is expected to be
integrated within the present project. An important question throughout the
remainder of this document is accordingly the extent to which automation may
enhance either the development or implementation of the various stages
considered below. A good example for the effectiveness of automation in the
kinds of peer review processes envisioned to emerge from this process is
provided by submissions to the [Journal of Open Source
Software](https://joss.theoj.org/), which features [open
reviews](https://github.com/openjournals/joss-reviews/issues), many aspects of
which are automated by a custom-developed bot called
["whedon"](https://github.com/whedon).


## 1.1 Project Goals:

  - To foster a community of practice in which users and developers of
    statistical software mutually improve quality, reproducibility, and
    reliability of research.
  - To provide software creators with a set of tools to assess the quality of
    their work, and a process by which to improve it.
  - To provide users of statistical software a discoverable "badge" that
    transparently conveys a level of assurance of software quality.
  - To create a set of standards that may be adopted and adapted by open source
    and private groups, academic journals, or other statistical software
    evaluation projects.
  - To focus on R as primary language, but separate language-specific from
    language- agnostic components so as to maximize adaptability to other
    contexts
  - To focus on problems and solutions specific to statistical software

---

# 2. Scope: What counts as "statistical software" in the context of peer review

There is no ready definition for "statistical software", but nor may such
a definition be essential to the success of the present project. An
[analysis](https://github.com/mpadge/statistical-software/tree/master/jss)
conducted in preparing this document of all historical submissions to the
Journal of Statistical Software revealed a notable *homogeneity* in textual
descriptions of software, with no notable phrases or topics which might be
useful to define or categorise statistical software, other than topics
pertaining to particular areas of application. That journal defines [its
own scope](https://www.jstatsoft.org/pages/view/mission) as, 

> statistical computing in all areas of empirical research.

It may suffice, at least initially, to simply provide broad definitions of what
might lie *beyond* the scope of the present project, or even to assert that
statistical software must at heart implement some kind of statistical algorithm
(or perhaps *employ* or otherwise interface with such in sufficiently novel
form). This is nevertheless a categorical exercise, and so one way or another we anticipate having to address the fundamental quesion(s) of:

1. What categories of statistical software might be considered *in scope*?
2. What categories of statistical software might be considered *out of scope*?

More specific questions of scope which will also need to be addressed include
the potential acceptability of the following different external forms of
software:

  - R packages
  - Other forms of bundling or presenting R code other than standard packages
  - Formats appropriate for other languages (such as python) yet with some
    interface with the R language
  - Independently-developed software in different languages which the
    submission exposes to the R environment ("wrapper" packages)

It considering both of these issues of definition, it will be important to
consider whether it may be advantageous or necessary to develop different
procedures for different categories, whether in terms of categories of external
form or categories of statistical software.


---

# 3. Review Process

Peer review as currently implemented by rOpenSci effectively functions in
a manner directly analogous to more conventional peer review of academic
manuscripts: A piece of software is submitted for review and, once accepted, is
"published" by rOpenSci, with publication symbolized both through a badge
system, and through transferring the software from an author's private domain
to the [`github.com/ropensci` domain](https://github.com/ropensci). It is very
important for the current project to note that this is only one of many
potential models for peer review of software, and particularly that a long
history and tradition in both practice and published literature on software
review [for example, @mili_software_2015; @ammann_introduction_2017] generally
concludes that software review is most effective when it is an ongoing process
that occurs as frequently as possible---a practice which contrasts strongly with
the singular nature of review as currently implemented by rOpenSci.

An effective system for peer review of statistical software is thus likely to
lie somewhere between current "one-off" practices typical of academic
manuscripts, and frequent, ongoing review typical of software development. The
abiding question to be considered within the present section is thus how the
review process might best be structured, organized, and managed.

There is obviously a direct analogy with the review process as developed by
rOpenSci. That process has itself developed and changed over the years since
its inception, through progressively introducing more formal structures
ultimately leading to [rOpenSci's Guide to *Development, Maintenance and Peer
Review* of R packages](https://devguide.ropensci.org/). The history of
development leading to these formalised guidelines can be followed through
extracting the histories of all rOpenSci packages, along with the histories of
the associated [software-review](https://github.com/ropensci/software-review)
issues. Such an analyses was conducted by staff member [MaÃ«lle
Salmon](https://github.com/maelle), and presented in a series of [rOpenSci blog
entries](https://ropensci.org/blog/2018/04/26/a-satrday-ct-series/).

## 3.1 Key considerations

  - Are we reviewing full packages or only limited pieces of packages?
  - What is the outcome of review? Binary, rating, checklist,
    acceptance/rejection?
  - To what extent should the review process be automated or self-certified?
  - Reviewer pool and qualifications:
    - What is the extent and type of effort expected of reviewers?
    - To what extent might searches for suitable reviewers be automated?
    - What sort of metrics might be useful in such searches?
  - Which parts of the process should be open and which closed?
  - Should review be a one-off phenomenon, or should there be multiple review
    phases throughout the software lifecycle?
  - Might there be advantage in considering "micro-review" or inline systems
    like [`watson-ruby`](https://github.com/nhmood/watson-ruby)
    [@spencer_open-source_2015]?
  - If multiple review phases, should there also be different reviewers for
    different phases?
  - Is it likely to be important to maintain the separation or independence of
    reviewers from code development, or might it be better to encourage direct
    engagement of reviewers with ongoing code development?

---

# Standards

  - General and language-specific software standards
  - Standards specific to statistical software
  - To what extent should we aim for "verification" or "validation" of software?

## Statistical

   - Numerical issues
   - Method validity (i.e., is the method itself valid, independent of
     implementation? Has to do with, perhaps, whether there's literature
     supporting the method.)
   - Scope of applicability of the software / method
   - Is a submission intended to support future or subsequent publications?

## Software Interface

There are likely aspects of overall software "design" that might be considered,
reviewed, encouraged, or expected. rOpenSci's guide on [package development,
maintenance, and peer review](https://devguide.ropensci.org/) provides arguably
one of the most prominent guides on the design of R packages, primarily with
its first chapter. One of the few other notable examples of guides to design
principles of R packages is the [tidyverse design
guide](https://principles.tidyverse.org/). The following useful list of design
principles is provided by @mili_software_2015:

1. Functional Attributes
    - Correctness
    - Robustness
2. Useability Attributes
    - Ease of Use
    - Ease of Learning
    - Customizability
    - Calibrability
    - Interoperability
3. Structural Attributes
    - Design Integrity
    - Modularity
    - Testability
    - Adaptability

### Key Considerations

  - Might there be an advantage to explicitly considering some of these aspects
    of general software design?
  - If so, which?
  - At what point in the general lifecycle of software or review might such
    design aspects best be considered or integrated?

## Documentation

In contrast to scientific, software in general, statistical software might be
argued to be associated with some specific kinds of input and output data. This
observation alone suggests the likely importance of some kind of metadata
associated with statistical software which documents the nature of (expected,
permitted, or accepted) input and output data [@lenhardt_data_2014].

  - Standard software documentation metrics:
    - Numbers of lines per function
    - proportion of documentation to code lines
    - presence of examples
    - coverage of examples
    - vignettes

## Testing

@vogel_medical_2011 states: Software that

> depends on testing alone for a defect-free [state] is depending on perfection
in testing

There are no unambiguous categories of tests, but the structure of R packages
which contain both external (exported) and internal (non-exported) functions
provides for a convenient distinction between "unit tests" as tests of
*internal* functions, and *functional* or *integration tests* as tests of
*exported* functions. Almost all testing as currently implemented in R is
"concrete testing" [@mili_software_2015], and little consideration has been
given in R to "stochastic" or "property-based" testing, in which expectation
values of inputs and outputs are tested, rather than concrete instantiations of
such. Other languages have developed grammars for stochastic or property-based
testing, notably through the [`hypothesis` package for
python](https://github.com/HypothesisWorks/hypothesis). These grammars enable
specification of test assumptions as well as expected test outputs. Assumptions
in `hypothesis` are declared through simple `@given` statements that might, for
example, quantify an assumed probability distribution for input data, while
outputs are specified through equivalent `@expect` statements that might, for
example, specify expected *distributional properties* of an output rather than
just concrete values.

### Key considerations

  - To what extend should testing focus on *functional* rather than *unit*
    testing?
  - What test reporter should be used? The `testthat` package and similar, or
    might it be worth considering new test reporting systems?
  - Is it sufficient to consider test execution as an integral part of
    `R CMD check` only? Or might there by a case for developing alternative,
    potentially longer-running, test execution environments?
  - Is it worthwhile concretely defining one or more goals of testing? (Such as
    error detection, error frequencies, error tolerance, accuracy.)
  - What are the test data? And how easy is it to input alternative data to
    tests?
  - Is there scope for "stochastic" or "property-based" testing?


# Tools for Evaluation and Review

  - What tools should we focus on developing?
  - What metrics or reports are useful to authors and reviewers?
  - What metrics or measures should be the basis for standards, in absolute
    or relative terms?

## Metrics

-  Code structure
    - Cyclomatic complexity
    - Codebase size
    - Function size / number
    - Numbers of external calls within functions
    - Exported / non exported functions
    - Code consistency
-  Documentation metrics detailed above
- Meta struture
    - Dependencies
    - Reverse dependencies
    - License
-  Meta metrics
    - Version control?
    - Availability of website
    - Availability of source code (beyond CRAN or similar)
    - Community:
        - Software downloads and usage statistics
        - Numbers of active contributors
        - Numbers or rates of issues reported
    - Maintenance:
        - Rate/Numbers of releases
        - Rate of response to reported issues
        - Last commit
        - Commit rate
    - stars
    - forks
-  Extent of testing
    - Code coverage
    - Examples
    - Range of inputs tested
- Dynamic metrics derived from function call networks

## Diagnostic reports 

  - Extensions of packages such as **lintr**, **covr**, **goodpractice**
  - Comparisons of package metrics to distributions for other packages
  - Diagnostic and report aggregation, design, automatic creation

---

# Related Frameworks and Projects

## Risk-Based Validation

   -  R Validation Hub work

---

# Annotated Bibliography

Lots to put here, but some in addition to stuff in current document:

   -  https://www.alexpghayes.com/blog/type-stable-estimation/, https://www.alexpghayes.com/blog/testing-statistical-software/
   -  https://tidymodels.github.io/model-implementation-principles/
   -  https://github.com/pharmaR/white_paper
   -  https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/criteria.md
