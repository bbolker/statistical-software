---
title: "rOpenSci Statistical Software Review: Scoping Document"
author: "Noam Ross and Mark Padgham"
date: "`r Sys.Date()`"
output: 
    html_document:
        toc: true
        toc_float: true
        number_sections: false
        theme: flatly
bibliography: statsoft.bib
---

<!-- uncomment to render title in github markdown
# rOpenSci Statistical Software Review: Scoping Document
-->

```{r getbib, echo = FALSE}
if (!file.exists ("statsoft.bib")) {
    refs <- RefManageR::ReadZotero(group = "2416765",
                                  .params = list (limit = 100))
    RefManageR::WriteBib(refs, "statsoft.bib")
}
```

# 1. Introduction

This document primarily serves to pose a number of questions regarding
rOpenSci's incipient project for peer-review of statistical software. The
organization itself is, and is likely to largely remain, primarily focussed on
the R language for statistical computing. While the project under consideration
here will likely evolve towards a system for peer review of *R packages*, it
may also consider review of other forms for bundling R software, or software
primarily written in other languages. Moreover, R packages often contain code
from a variety of other languages, traditionally Fortran and C, now very
commonly also C++, as well as other non-compiled languages such as javascript,
and compiled languages such as Rust. It is accordingly expected that software
review will encompass several languages, whether or not bundled in the form of
R packages. Moreover, the project aims to develop a set of language-independent
standards able to be transferred to other languages and systems for peer
review.

Peer-review plays a key role in ensuring the sustainability of scientific
software [@downs_community_2015]. rOpenSci has a well-developed system for peer
review of R packages, primarily through their ["software-review" repository on
github](https://github.com/ropensci/software-review), to which packages may be
submitted by opening an issue on that github repository. The review process is
entirely open, with each issue used to manage the entire process, coordinated
by rOpenSci's own editorial committee. Two features of this peer review system
in its current state are important to note here:

1. Statistical software is explicitly *excluded* from current scope; and
2. Tasks associated with peer review are not automated, and require manual
   control and determination of each step.

There are many stages associated with software development and peer-review that
may effectively be automated, and aspects of such automation is expected to be
integrated within the present project. An important question throughout the
remainder of this document is accordingly the extent to which automation may
enhance either the development or implementation of the various stages
considered below. A good example for the effectiveness of automation in the
kinds of peer review processes envisioned to emerge from this process is
provided by submissions to the [Journal of Open Source
Software](https://joss.theoj.org/), which features [open
reviews](https://github.com/openjournals/joss-reviews/issues), many aspects of
which are automated by a custom-developed bot called
["whedon"](https://github.com/whedon).


## 1.1 Project Goals:

  - To foster a community of practice in which users and developers of
    statistical software mutually improve quality, reproducibility, and
    reliability of research.
  - To provide software creators with a set of tools to assess the quality of
    their work, and a process by which to improve it.
  - To provide users of statistical software a discoverable "badge" that
    transparently conveys a level of assurance of software quality.
  - To create a set of standards that may be adopted and adapted by open source
    and private groups, academic journals, or other statistical software
    evaluation projects.
  - To focus on R as primary language, but separate language-specific from
    language- agnostic components so as to maximize adaptability to other
    contexts
  - To focus on problems and solutions specific to statistical software

---

# 2. Scope: What counts as "statistical software" in the context of peer review

There is no ready definition for "statistical software", but nor may such
a definition be essential to the success of the present project. An
[analysis](https://github.com/mpadge/statistical-software/tree/master/jss)
conducted in preparing this document of all historical submissions to the
Journal of Statistical Software revealed a notable *homogeneity* in textual
descriptions of software, with no notable phrases or topics which might be
useful to define or categorise statistical software, other than topics
pertaining to particular areas of application. That journal defines [its
own scope](https://www.jstatsoft.org/pages/view/mission) as, 

> statistical computing in all areas of empirical research.

It may suffice, at least initially, to simply provide broad definitions of what
might lie *beyond* the scope of the present project, or even to assert that
statistical software must at heart implement some kind of statistical algorithm
(or perhaps *employ* or otherwise interface with such in sufficiently novel
form). This is nevertheless a categorical exercise, and so one way or another we anticipate having to address the fundamental quesion(s) of:

1. What categories of statistical software might be considered *in scope*?
2. What categories of statistical software might be considered *out of scope*?

More specific questions of scope which will also need to be addressed include
the potential acceptability of the following different external forms of
software:

  - R packages
  - Other forms of bundling or presenting R code other than standard packages
  - Formats appropriate for other languages (such as python) yet with some
    interface with the R language
  - Independently-developed software in different languages which the
    submission exposes to the R environment ("wrapper" packages)

It considering both of these issues of definition, it will be important to
consider whether it may be advantageous or necessary to develop different
procedures for different categories, whether in terms of categories of external
form or categories of statistical software.


---

# 3. Review Process

Peer review as currently implemented by rOpenSci effectively functions in
a manner directly analogous to more conventional peer review of academic
manuscripts: A piece of software is submitted for review and, once accepted, is
"published" by rOpenSci, with publication symbolized both through a badge
system, and (generally) through transferring the software from an author's
private domain to the [`github.com/ropensci`
domain](https://github.com/ropensci). It is very important with regard to the
current project to note that this is only one of many potential models for peer
review of software, and particularly that a long history and tradition in both
practice and published literature on software review [for example,
@mili_software_2015; @ammann_introduction_2017] generally concludes that
software review is most effective when it is an ongoing process that occurs as
frequently as possible---a practice which contrasts strongly with the singular
nature of review as currently implemented by rOpenSci.

An effective system for peer review of statistical software is thus likely to
lie somewhere between current "one-off" practices typical of academic
manuscripts, and frequent, ongoing review typical of software development. An
[analysis of the effects of rOpenSci's review process on a few metrics of
software development
activity](https://github.com/mpadge/statistical-software/tree/master/ros-review-effects)
revealed that software development tends to stagnate following review. While
this might be interpreted to reflect software having reached a sufficiently
stable state, we note that metrics of community engagement with software are
generally positively related to the metrics of development activity considered
there, and thus that these kinds of empirical decreases in rates of software
development following review are likely associated with concomitant,
progressive decreases in community engagement. The abiding question to be
considered within the present section is thus how the review process might best
be structured, organized, and managed, keeping such potentially negative "side
effects" in mind.


## 3.1 Key considerations

  - Are we reviewing full packages or only limited pieces of packages?
  - What is the outcome of review? Binary, rating, checklist,
    acceptance/rejection?
  - To what extent should the review process be automated or self-certified?
  - Reviewer pool and qualifications:
    - What is the extent and type of effort expected of reviewers?
    - To what extent might searches for suitable reviewers be automated?
    - What sort of metrics might be useful in such searches?
  - Which parts of the process should be open and which closed?
  - Should review be a one-off phenomenon, or should there be multiple review
    phases throughout the software lifecycle?
  - Might there be advantage in considering "micro-review" or inline systems
    like [`watson-ruby`](https://github.com/nhmood/watson-ruby)
    [@spencer_open-source_2015]?
  - If multiple review phases, should there also be different reviewers for
    different phases?
  - Is it likely to be important to maintain the separation or independence of
    reviewers from code development, or might it be better to encourage direct
    engagement of reviewers with ongoing code development?

---

# 4. Standards

An important output of the present project is anticipated to be two set of
"standards", one pertaining to processes for peer review of software, and one
pertaining to the software itself. The former of these is only indirectly
addressed within this document, through the preceding sections, with concrete
standards expected to emerge from latter phases of the project. The latter
aspect of software standards is the subject of the present section.

Rather than aiming for a fixed set of immutable standards, we acknowledge that
standards of the kind anticipated here will likely be better conceived of to
reflect ongoing processes of development. As such, of equal importance to
developing a set of standards *per se* will be developing an understanding of
the kinds of *processes* which may have the most defining effect on resultant
standards at any point in time. An equally important consideration is the
distinction between pro- and retro-spective standards, where *retrospective
standards* can be applied to any extant software in order to assess the extent
to whether it meets those standards, while *prospective standards* represent
a set of requirements which software is expected to fulfil. The extent to which
standards developed for this project are retrospective versus prospective is
likely to change throughout the project's lifespan and beyond.

Standards are likely to emerge from a variety of concrete metrics of the types
considered in the following section ("*Specific Aspects of Software*"), and are
not considered in any technical detail in the present section. Important
*general* questions regarding standards include the following:

  - What kind of standards might apply to software in general?
  - How might such standards differ between different languages?
  - What kind of standards might specifically apply to statistical software?
    (See the following sub-section.)
  - In what ways might any of the above differ with regard to retro- versus
    pro-spective standards?
  - To what extent should we aim for "verification" or "validation" of
    software?

## 4.1 Statistical Standards

   - Numerical issues
   - Method validity (i.e., is the method itself valid, independent of
     implementation? Has to do with, perhaps, whether there's literature
     supporting the method.)
   - Scope of applicability of the software / method
   - Is a submission intended to support future or subsequent publications?

## 4.2 Standards and Software Categories

Many of the metrics described in the following section can be "automatically"
quantified, and thus procedures for assessing the extent to which software
meets a given sets of standards are readily automated. Doing so nevertheless
entails some degree of risk which it is important both to apperceive, and to
consider within the present general context of standards. Such risk is
particularly apparent with regard to the relationship between software
categories and standards---although these kinds of considerations of risk are in
no way restricted to this domain only.

The applicability of any concrete set of standards is likely to differ between
different categories. In terms of concrete metrics, for example, metrics of
numerical accuracy will differ between categories primarily describing
analytical algorithms and those describing less tractable routines which
produce less directly reproducible results. Or consider metrics derived from
tests, which must be interpreted in *qualitatively* different ways for packages
entirely dependent on their own internal code versus packages largely dependent
on the results of calls to external data providers (along with additional
differences between, for example, locally-installed "external" providers versus
online sources of external data).

Different metrics, and thus by extension very likely different standards, must
thus be considered to be of differential applicability for different categories
of software, and thus the interplay between the scope of statistical software
considered above and throughout this project, and the standards emerging from
the project, will be of critical importance throughout the project. More
concretely, attempts to devise a concrete categorization of statistical
software will inevitably frustrate attempts to understand more *general*
processes leading to the establishment of standards, and therefore negatively
impact on the project's desired goal stated at the outset of creating and
providing a truly *general* and *transferrable* set of standards. Conversely,
attempts to counter this effect by striving for more generally applicable
standards will likely weaken abilities to assess particular categories of
statistical software. Such considerations lead to the following kinds
of questions which will likely have to be addressed:

  - To what extent ought we aim for general standards at the expense of
    specific ability to assess particular categories of statistical software?
  - To what extent ought we strive for automation of software assessment, given
    the inherent risk of overseeing qualitative differences between different
    categories?
  - How much effort should be expended both developing a categorization of
    statistical software, and understanding the potential effects of such
    a categorization?

The ultimate question which we would like to answer throughout such
considerations concerns the kinds of tools this project might best focus on
developing, whether those tools be:

  - Qualitative descriptions and understanding of effects of the kinds of
    compromises described above
  - Tools useful in assessing or formalizing categories of software
  - Quantitative tools to *retrospectively* assess such aspects as:
    - Software "quality" (see the subsequent section)
    - Community engagement
    - Effectiveness (or other metrics) of review
    - Other aspects pertinent to the general process of peer reviewing open
      source statistical software
  - Quantitative tools that can be *prospectively* used to
    - Improve or assure software quality
    - Document aspects of software quality
    - Aid modularity or transferability either of software, or of the tools
      themselves
    - Formalize structural aspects of software such as tests (for example,
      through implementing new frameworks or grammars)


# 5. Specific Aspects of Software

To additionally inform the general considerations described in the preceding
sections, this section presents a general (yet non-exhaustive) overview of the
kinds of metrics which may be usefully considered in assessing statistical
software, and which may be usefully employed in deriving a set of standards for
such software. This section is primarily intended to address the two directly
related questions of the extent to which the kinds of metrics described might
be useful to both software developers, and to reviewers.

## 5.1 General Software Interface

There are likely aspects of overall software "design" that might be considered,
reviewed, encouraged, or expected. rOpenSci's guide on [package development,
maintenance, and peer review](https://devguide.ropensci.org/) provides arguably
one of the most prominent guides on the design of R packages, primarily with
its first chapter. One of the few other notable examples of guides to design
principles of R packages is the [tidyverse design
guide](https://principles.tidyverse.org/). The following useful list of design
principles is provided by @mili_software_2015:

1. Functional Attributes
    - Correctness
    - Robustness
2. Useability Attributes
    - Ease of Use
    - Ease of Learning
    - Customizability
    - Calibrability
    - Interoperability
3. Structural Attributes
    - Design Integrity
    - Modularity
    - Testability
    - Adaptability

### 5.1a Key Considerations

  - Might there be an advantage to explicitly considering some of these aspects
    of general software design?
  - If so, which?
  - At what point in the general lifecycle of software or review might such
    design aspects best be considered or integrated?

## 5.2 Documentation

In contrast to scientific, software in general, statistical software might be
argued to be associated with some specific kinds of input and output data. This
observation alone suggests the likely importance of some kind of metadata
associated with statistical software which documents the nature of (expected,
permitted, or accepted) input and output data [@lenhardt_data_2014].

  - Standard software documentation metrics:
    - Numbers of lines per function
    - proportion of documentation to code lines
    - presence of examples
    - coverage of examples
    - vignettes

## 5.3 Testing

@vogel_medical_2011 states: Software that

> depends on testing alone for a defect-free [state] is depending on perfection
in testing

There are no unambiguous categories of tests, but the structure of R packages
which contain both external (exported) and internal (non-exported) functions
provides for a convenient distinction between "unit tests" as tests of
*internal* functions, and *functional* or *integration tests* as tests of
*exported* functions. Almost all testing as currently implemented in R is
"concrete testing" [@mili_software_2015], and little consideration has been
given in R to "stochastic" or "property-based" testing, in which expectation
values of inputs and outputs are tested, rather than concrete instantiations of
such. Other languages have developed grammars for stochastic or property-based
testing, notably through the [`hypothesis` package for
python](https://github.com/HypothesisWorks/hypothesis). These grammars enable
specification of test assumptions as well as expected test outputs. Assumptions
in `hypothesis` are declared through simple `@given` statements that might, for
example, quantify an assumed probability distribution for input data, while
outputs are specified through equivalent `@expect` statements that might, for
example, specify expected *distributional properties* of an output rather than
just concrete values.

### 5.3a Key considerations

  - To what extend should testing focus on *functional* rather than *unit*
    testing?
  - What test reporter should be used? The `testthat` package and similar, or
    might it be worth considering new test reporting systems?
  - Is it sufficient to consider test execution as an integral part of
    `R CMD check` only? Or might there by a case for developing alternative,
    potentially longer-running, test execution environments?
  - Is it worthwhile concretely defining one or more goals of testing? (Such as
    error detection, error frequencies, error tolerance, accuracy.)
  - What are the test data? And how easy is it to input alternative data to
    tests?
  - Is there scope for "stochastic" or "property-based" testing?


## 5.4 Metrics

-  Code structure
    - Cyclomatic complexity
    - Codebase size
    - Function size / number
    - Numbers of external calls within functions
    - Exported / non exported functions
    - Code consistency
-  Documentation metrics detailed above
- Meta struture
    - Dependencies
    - Reverse dependencies
    - License
-  Meta metrics
    - Version control?
    - Availability of website
    - Availability of source code (beyond CRAN or similar)
    - Community:
        - Software downloads and usage statistics
        - Numbers of active contributors
        - Numbers or rates of issues reported
    - Maintenance:
        - Rate/Numbers of releases
        - Rate of response to reported issues
        - Last commit
        - Commit rate
    - stars
    - forks
-  Extent of testing
    - Code coverage
    - Examples
    - Range of inputs tested
- Dynamic metrics derived from function call networks

## 5.5 Diagnostic reports 

  - Extensions of packages such as **lintr**, **covr**, **goodpractice**
  - Comparisons of package metrics to distributions for other packages
  - Diagnostic and report aggregation, design, automatic creation

---

# Related Frameworks and Projects

## Risk-Based Validation

   -  R Validation Hub work

---

# Annotated Bibliography

Lots to put here, but some in addition to stuff in current document:

   -  https://www.alexpghayes.com/blog/type-stable-estimation/, https://www.alexpghayes.com/blog/testing-statistical-software/
   -  https://tidymodels.github.io/model-implementation-principles/
   -  https://github.com/pharmaR/white_paper
   -  https://github.com/coreinfrastructure/best-practices-badge/blob/master/doc/criteria.md
